---
layout: post
title:  ""
date:   
categories: economics, politics, technology, science, philosophy
---
<!---
median of singularity forecasts = 2040

median of brain computing power = 10ˆ18 FLOPS
    http://aiimpacts.org/brain-performance-in-flops/
    power efficiency = 10^18 FLOPS / 20W = 10^16 FLOPS/W
current top supercomputer = 10^17 FLOPS
    power efficiency = 10ˆ9 FLOPS/W (100 MW, 70% of Recife household power consumption, Itaipu would power only 140 of those)
    size = 40 cabinets

[book] humans need not apply, 2015
[book] rise of the robots, 2015

other job anxiety waves:
    industrial revolution, séc. 18/19
    "technological unemployment", keynes, primeira metade do séc. 20
    first business computers and industrial robots, 60s
    first personal computers, 80s

recent ai headlines:
    ibm deep blue defeats kasparov, 1997
    ibm watson defeats jeopardy champions, 2011
    imagenet challenge 2012 (10% jump to 85% accuracy against human average 95%, now "super" human 97.7%)
    alphago defeats lee sedol 4-1, 2016

socio-economic evolution:
    1800 <4% urbanization
    2006 >50%
    2014 >80%

there is a sense that convergence of technological advances (ai, robotics, iot, crispr, nano, crypto...) is bringing a 4th industrial revolution with big promisses and big threats

all of those developments are expected to impact deeply the way we live, but the biggest elephant in the room probably from various perspectives but certainly from a labour market one seems to be ai/robotics

And that's understandable because ai brings with it the prospect of human labour replacement, at minimum, and complete human obsolescence ultimately. are those concerns rooted in reality?

i will focus on the technological aspects and leave the socio-economical, at least on this first pass, to my colleagues on this panel. what (if any) are the technology foundations of this new wave of both euphoria and anxiety?

robocalypse (human level ai) is still far ahead (singularity forecasts, computing power still not there, it's not just about flops, open/complex real world requires learning, architecture/paradigm change to networks, simulation and physical realization, simulation energy efficiency still very far from brain, neuromorphic hardware still very far from brain, network sizes still bee-size, human size forecast around 2050, but possibly not enough due to brain node complexity).

but u don't need human level ai to disrupt the labour markets (specific tasks may be took over by bees)

and recent advances in ai, especially deep learning, give good reason for concern for those on the pessimistic side (techniques, large data sets, hardware => results)

the truth is technology does kill jobs - while at the same time create others. and we are historically very bad at predicting what new jobs will be created. will things be net positive this time as before? optimists are reasonably extrapolating from the past while pessimists have a point that there is now gurantee the future will repeat the past. are the bees coming for your job?
-->
Existe essa excitação enorme no momento em relação aos avanços tecnológicos em diversas áreas, uma mistura de euforia e ansiedade (em alguns casos até pânico). Todas essas frentes de inovação tecnológica (IA, Robótica, Internet das Coisas, Edição Genética, Nanotecnologia...) devem impactar profundamente, ao longo das próximas décadas, a maneira que vivemos, nos organizamos em sociedade (aí incluído obviamente o trabalho) e até como vemos a nós mesmos como indivíduos e espécie. Todas elas carregam potencialmente grandes oportunidades de melhoria da condição humana e ao mesmo tempo de consequências não-intencionais catastróficas.

Mas a mim parece claro que a mais fundamental dessas frentes, sob diversas perspectivas, mas certamente sob a perspectiva do trabalho, é IA/Robótica. Ela é o maior elefante na sala na visão daqueles que estão do lado pessimista (ou seria realista?) do debate sobre a chamada quarta revolução industrial.

E isso é compreensível uma vez que a IA traz consigo o prospecto de substituição do trabalho humano em todos os níveis e, em última instância, da própria obsolescência do ser humano como agente da geração de riqueza (ou obsolescência pura e simples do humano).

Mas essas previsões pessimistas têm fundamento na realidade?

Eu vou focar nos aspectos tecnológicos de IA e deixar os sócio-econômicos para os meus companheiros de painel, pelo menos nessa primeira passagem. Ou seja, quais são os fatos tecnológicos que fundamentariam essa nova onda tanto de euforia quanto de ansiedade em realação às tecnologias de automação?

A mediana das previsões para a chamada singularidade ou o "robocalipse" (depende do seu grau de otimismo/pessimismo) aponta hoje para algum momento na década de 2040. Esse seria o momento em que um sistema de IA atingiria superinteligência, ou inteligência super-humana, não em algumas tarefas específicas (o que já é realidade há décadas), mas em perceber e interpretar o mundo real e atuar sobre ele (a chamada inteligência geral artificial). Em geral essas previsões se baseiam em estimativas da capacidade computacional do cérebro humano e na extrapolação pura e simples da curva de crescimento histórica da capacidade de processamento de sistemas artificiais.

A mediana das estimativas atuais para a capacidade do cérebro humano, de acordo com algumas fontes respeitáveis, está na ordem de 1 ExaFLOPS ou 1 bilhão de bilhões de operações de ponto flutuante por segundo (10^18). Para efeito de comparação, um smartphone moderno está na ordem de GFLOPS (10^9, um bilionésimo dessa capacidade). Mas o supercomputador mais rápido do mundo, o chinês _Sunway TaihuLight_, conta com mais de 40 mil processadores RISC interligados, cada um com 256 núcleos, e atinge desempenho máximo de 93 PetaFLOPS (pouco menos de 10% dos 1 HexaFLOPS estimados para o nosso cérebro). Nada mal, hein? Pelas estimativas atuais, teremos um sistema Exascale entre 2020 e 2023. O primeiro sistema computacional com capacidade estimada equivalente ao cérebro humano. Bem antes de 2040.

Acontece que não é só uma questão de capacidade bruta de processamento. Tão ou mais importantes são a arquitetura do sistema (quais os componentes, como eles se relacionam uns com os outros), os princípios físicos de operação, os materiais e o paradigma de programação. Um exemplo das implicações críticas dessas questões é a eficiência energética: Com a arquitetura de hardware utilizada atualmente, o _Sunway Taihulight_ necessita de 100 Megawatts de potência, equivalente a 70% de todo o consumo residencial médio da cidade do Recife!, para atingir menos de 10% da capacidade computacional de um cérebro humano. Um sistema Exascale construído hoje consumiria o equivalente a toda a cidade de São Paulo (residencial)! Seria necessário uma usina hidroelétrica ou um reator nuclear só para alimentá-lo e gerar o processamento de menos de um kilo e meio de gosma que não consome mais que duas lâmpadas de LED (20W). Isso para não falar de espaço físico e outros custos de infraestrutura e operação. Com uma competição dessas estamos tranquilos =)

Mas capacidade computacional e custo são apenas parte da estória. De nada adianta ExaFLOPS de capacidade se você não sabe o quê computar nem como computar. O objetivo final da inteligência artificial é produzir o equivalente à nossa capacidade de interpretar, prever e manipular o mundo. E cada vez mais a comunidade científica se convence de que especificar formalmente todo o conhecimento necessário para isso (o paradigma de programação tradicional, baseada em regras) não é uma abordagem viável. A complexidade do mundo real exige capacidade de o sistema aprender continuamente através da experiência. Essa é a abordagem conhecida genericamente como aprendizagem de máquina (_machine learning_). Nesse campo nós temos feito avanços significativos, especialmente nos últimos anos, com modelos computacionais inspirados na nossa biologia, as chamadas redes neurais artificiais, e especialmente as redes neurais profundas (a famosa _deep learning_). As redes neurais não são uma novidade e já estão por aí desde os primórdios da ciência da computação nos anos 40. Mas avanços recentes nas técnicas e no hardware, assim como a disponibilidade de grandes conjuntos de dados como subproduto da digitalização em massa provocada pela Internet, estão propiciando resultados impressionantes nessa última década. E com eles todo esse _buzz_ na mídia e na sociedade como um todo.

Em termos bem simplificados, uma rede neural artificial é um sistema composto de um grande número de pequenos elementos computacionais encadeados de forma que as saídas de uns viram entradas de outros. Grosseiramente, é análogo à maneira que os nossos neurônios são interligados no cérebro. Um neurônio, biológico ou artificial, só produz algo na saída quando a combinação das suas entradas ultrapassa um determinado limiar, o que chamamos de disparo. Ajustando esse limiar e também o peso dado a cada uma das entradas na combinação, em cada um dos neurônios, é possível fazer uma rede dessas realizar computações as mais diversas. E, muito mais importante, existem algoritmos que permitem ao sistema calcular automaticamente os valores desses pesos e limiares à partir dos dados observados, com e sem auxílio humano. Ou seja, a rede aprende com a experiência. Pronto, acabei de dar um _crash course_ pra vocês em redes neurais artificiais =)

O cérebro humano possui algo em torno de 1 trilhão de neurônios (20 bilhões apenas no córtex) e quatrilhões de sinapses. As sinapses são as junções entre os neurônios biológicos e correspondem às conexões entre neurônios artificiais. Enquanto o cérebro humano implementa sua rede neural diretamente em "hardware" (a gente gosta de chamar de "wetware"), a abordagem amplamente dominante em redes neurais artificiais é a simulação em software. De uma forma geral, quanto maior a rede em quantidade de neurônios e conexões, maior a acurácia e a complexidade da tarefa que ela é capaz de aprender e de executar. As maiores redes neurais artificiais implementadas até hoje estão na ordem de alguns milhões de neurônios, mais para uma abelha do que para um sapo. Isso se considerarmos que a complexidade das funções implementadas por neurônios biológicos individuais é similar à dos neurônios artificiais. Com essa premissa e considerando a taxa de crescimento histórica, não chegaremos a redes de escala humana antes de 2050. Adicione na conta a gigantesca ineficiência das atuais arquiteturas de hardware (como comentado anteriormente e que pode desacelerar essa curva de crescimento) e ainda a sobrecarga representada pela simulação ao invés de uma implementação direta e a década de 2040 parece ser uma estimativa super-otimista (ou super-pessimista) para a singularidade (apocalipse robô).

Entretanto, acabam aqui as boas notícias para os distópicos e as más notícias para os utópicos. Existem esforços internacionais significativos em andamento ou em gestação para o desenvolvimento de arquiteturas de hardware neuromórficas, que nada mais são que tentativas de combinar o melhor das arquiteturas de hardware artificiais desenhadas pelo homem com o melhor das arquiteturas orgânicas geradas pela evolução biológica para produzir uma nova arquitetura artificial bio-inspirada. É bem provável que esses esforços produzam avanços disruptivos na eficiência e escalabilidade dos sistemas computacionais nas próximas uma ou duas décadas.

Mas enquanto esses avanços em hardware não chegam, convém não esquecermos que abelhas são capazes de coisas incríveis. Não é necessário capacidade cognitiva em escala humana para realizar com sucesso tarefas humanas. Não é necessário inteligência artificial em nível humano para sacudir o mercado de trabalho. Boa parte das profissões não requerem um ser humano completo, muito pelo contrário. Alguns empregadores até gostariam da idéia de trabalhadores com menor escopo de atuação (e interesses). Pense em motoristas, vigilantes, ... Redes neurais profundas atuais já são capazes de feitos impressionantes: Desempenho "super-humano" no reconhecimento de imagens no Desafio ImageNet (97.7% contra 95% em média); Desempenho super-humano no reconhecimento de fala (); Desempenho super-humano em classificação de sinais de trânsito; Descrição de cenas capturadas em fotos; aprender a jogar video-games por tentativa e erro, sem assistência humana; prever interações de moléculas para ajudar a desenvolver novas drogas; vencer o maior jogador de Go do mundo por 4 a 1, um feito que não era esperado antes da próxima década...

A verdade é que a tecnologia de fato elimina postos de trabalho e ocupações - ao mesmo tempo que cria outros. E nós somos historicamente muito ruins em prever que novos tipos de trabalho serão criados. Sera que o saldo será positivo dessa vez como em todas as outras? Os otimistas estão, de forma razoável, extrapolando a partir do passado, enquanto os pessimistas têm razão em apontar que não há garantias de que o futuro repita passado. Estariam as abelhas vindo atrás do nosso trabalho?
